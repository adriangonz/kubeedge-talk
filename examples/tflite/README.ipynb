{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with TFLite on Edge devices\n",
    "\n",
    "This example walks through how to deploy a [TFLite model](https://www.tensorflow.org/lite/guide) in an edge device using Seldon Core.\n",
    "In particular, we will deploy a face mask detector in a Raspberry Pi, which will run inference locally from a camera feed.\n",
    "\n",
    "The face mask detector has already been pre-trained by the team at [Aizoo Tech team](https://aizoo.com) who has kindly put it available on the [AIZOOTech/FaceMaskDetection](https://github.com/AIZOOTech/FaceMaskDetection) repository. **Massive thanks to them!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seldon Core\n",
    "\n",
    "The example assumes that Seldon Core has already been installed in the cluster.\n",
    "If it hasn't, you can follow the [setup instructions in the Seldon Core documentation](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html).\n",
    "\n",
    "It's also worth noting that some steps below assume that Seldon Core has been installed using Helm with name `seldon-core` in the `seldon-system` namespace.\n",
    "This is not a hard requirement though, and it should be simple to adapt those steps in case it has been installed in a different location.\n",
    "\n",
    "### KubeEdge\n",
    "\n",
    "The example assumes that KubeEdge has already been installed in the cluster, exposing the `cloudcore` component so that it's accessible from an edge device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Device\n",
    "\n",
    "The example assumes that the edge device is a Raspberry Pi with an ARMv7 architecture of 32 bits.\n",
    "In particular, the example has been tested with a Raspberry Pi 3 Model B V1.2.\n",
    "\n",
    "We will assume that the device has been pre-synced through KubeEdge and that it's already visible as a node in the cluster with name `raspberry`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite Inference Server\n",
    "\n",
    "Out of the box, Seldon Core doesn't offer support for TFLite models (particularly under an ARM architecture).\n",
    "However, as we shall see, it's fairly simple to leverage their support for [custom inference servers](https://docs.seldon.io/projects/seldon-core/en/latest/servers/custom.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Model Runtime\n",
    "\n",
    "Firstly, we'll extend the `SeldonComponent` interface to create a new model runtime.\n",
    "The methods that we'll want to extend are:\n",
    "\n",
    "- `load()`: responsible for loading our TFLite model\n",
    "- `predict()`: responsible for running inference against our model\n",
    "\n",
    "Note that we also want to parametrise the following model-specific details:\n",
    "\n",
    "- Where are model weights loaded from (i.e. `model_uri`).\n",
    "- On which tensor should we load the input data (i.e. `input_tensor_name`).\n",
    "- From which tensor should we read the model's output (i.e. `output_tensor_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TFLiteServer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile TFLiteServer.py\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from seldon_core.user_model import SeldonComponent\n",
    "from tflite_runtime import interpreter as tflite\n",
    "from typing import List, Dict, Iterable\n",
    "\n",
    "TFLITE_EXT = \"*.tflite\"\n",
    "\n",
    "\n",
    "class TFLiteServer(SeldonComponent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_uri: str,\n",
    "        input_tensor_name: str,\n",
    "        output_tensor_name: str,\n",
    "    ):\n",
    "        self._model_uri = model_uri\n",
    "        self._input_tensor_name = input_tensor_name\n",
    "        self._output_tensor_name = output_tensor_name\n",
    "\n",
    "    def load(self):\n",
    "        model_path = self._get_model_path()\n",
    "        self._interpreter = tflite.Interpreter(model_path=model_path)\n",
    "        self._interpreter.allocate_tensors()\n",
    "\n",
    "        # Obtain input tensor index\n",
    "        input_tensors = self._interpreter.get_input_details()\n",
    "        self._input_tensor_index = self._get_tensor_index(\n",
    "            input_tensors, self._input_tensor_name\n",
    "        )\n",
    "\n",
    "        # Obtain output tensor index\n",
    "        output_tensors = self._interpreter.get_output_details()\n",
    "        self._output_tensor_index = self._get_tensor_index(\n",
    "            output_tensors, self._output_tensor_name\n",
    "        )\n",
    "\n",
    "    def _get_model_path(self) -> str:\n",
    "        # Search for *.tflite files to load\n",
    "        pattern = os.path.join(self._model_uri, TFLITE_EXT)\n",
    "        model_paths = glob.glob(pattern)\n",
    "        if not model_paths:\n",
    "            raise RuntimeError(f\"No models found at {self._model_uri}\")\n",
    "\n",
    "        return model_paths[0]\n",
    "\n",
    "    def _get_tensor_index(self, tensors: List[Dict], tensor_name: str) -> int:\n",
    "        for tensor in tensors:\n",
    "            if tensor[\"name\"] == tensor_name:\n",
    "                return tensor[\"index\"]\n",
    "\n",
    "        raise RuntimeError(f\"Tensor name not found: {tensor_name}\")\n",
    "\n",
    "    def predict(self, X: np.ndarray, names: Iterable[str], meta: Dict = None):\n",
    "        # Force input to be np.float32\n",
    "        img = np.float32(X)\n",
    "\n",
    "        # NOTE: This is not thread-safe!\n",
    "        self._interpreter.set_tensor(self._input_tensor_index, img)\n",
    "        self._interpreter.invoke()\n",
    "\n",
    "        output = self._interpreter.get_tensor(self._output_tensor_index)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building runtime image\n",
    "\n",
    "The next step will be building our runtime into a Docker image.\n",
    "It's worth mentioning that the new image has to be compatible with an ARM architecture, therefore we won't be able to use the [`s2i` facilities provided by Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/python/python_wrapping_s2i.html) out of the box.\n",
    "However, we can put up a together a `Dockerfile` based on [Seldon's documentation to wrap models using Docker directly](https://docs.seldon.io/projects/seldon-core/en/latest/python/python_wrapping_docker.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM arm32v7/python:3.7.8-slim\n",
    "\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "ENV API_TYPE=\"REST\"\n",
    "ENV SERVICE_TYPE=\"MODEL\"\n",
    "\n",
    "# Install native subdeps of Python deps.\n",
    "# - `libatlas`: necessary for `numpy`\n",
    "# - `libatomic1`: necessary for `grpcio`\n",
    "RUN apt-get update \\\n",
    "    && apt-get install -y libatlas-base-dev libatomic1\n",
    "\n",
    "RUN pip install --upgrade wheel setuptools pip\n",
    "\n",
    "COPY requirements.txt ./\n",
    "RUN pip install \\\n",
    "    -r requirements.txt \\\n",
    "    --extra-index-url=https://piwheels.org/simple\n",
    "\n",
    "COPY . .\n",
    "\n",
    "CMD seldon-core-microservice TFLiteServer $API_TYPE --service-type $SERVICE_TYPE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to build this image, we'll need a host able to build `arm` images.\n",
    "In Linux, we can achieve this using QEMU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.113MB\n",
      "Step 1/10 : FROM arm32v7/python:3.7.8-slim\n",
      " ---> 4eaf7bd6a6cb\n",
      "Step 2/10 : WORKDIR /usr/src/app\n",
      " ---> Using cache\n",
      " ---> a47231aaeda8\n",
      "Step 3/10 : ENV API_TYPE=\"REST\"\n",
      " ---> Using cache\n",
      " ---> 053e4eeb29c9\n",
      "Step 4/10 : ENV SERVICE_TYPE=\"MODEL\"\n",
      " ---> Using cache\n",
      " ---> 89c2a14b355d\n",
      "Step 5/10 : RUN apt-get update     && apt-get install -y libatlas-base-dev libatomic1\n",
      " ---> Using cache\n",
      " ---> 28529c0ee508\n",
      "Step 6/10 : RUN pip install --upgrade wheel setuptools pip\n",
      " ---> Using cache\n",
      " ---> 43983be6ea0b\n",
      "Step 7/10 : COPY requirements.txt ./\n",
      " ---> Using cache\n",
      " ---> 44bbd87e8594\n",
      "Step 8/10 : RUN pip install     -r requirements.txt     --extra-index-url=https://piwheels.org/simple\n",
      " ---> Using cache\n",
      " ---> 055557535b8e\n",
      "Step 9/10 : COPY . .\n",
      " ---> 8ff71208e314\n",
      "Step 10/10 : CMD seldon-core-microservice TFLiteServer $API_TYPE --service-type $SERVICE_TYPE\n",
      " ---> Running in 39fae836c850\n",
      "Removing intermediate container 39fae836c850\n",
      " ---> d6233970d6ba\n",
      "Successfully built d6233970d6ba\n",
      "Successfully tagged adriangonz/tfliteserver:0.1.0-arm\n",
      "The push refers to repository [docker.io/adriangonz/tfliteserver]\n",
      "\n",
      "\u001b[1B5f610190: Preparing \n",
      "\u001b[1B08a6c8b1: Preparing \n",
      "\u001b[1B65293277: Preparing \n",
      "\u001b[1Bf17926cb: Preparing \n",
      "\u001b[1Be22afbe4: Preparing \n",
      "\u001b[1B0f4fcd5e: Preparing \n",
      "\u001b[1Bedd599d4: Preparing \n",
      "\u001b[1B3354b755: Preparing \n",
      "\u001b[1Bf28abf87: Preparing \n",
      "\u001b[1B3a8fa994: Preparing \n",
      "\u001b[11Bf610190: Pushed lready exists 1MB\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K0.1.0-arm: digest: sha256:430d1cf962c327caf2aaeafad3c753ebace032009e35ee251e223ba43ad95467 size: 2631\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "    . \\\n",
    "    --platform linux/arm/v7 \\\n",
    "    -t adriangonz/tfliteserver:0.1.0-arm\n",
    "!docker push adriangonz/tfliteserver:0.1.0-arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new inference server to Seldon Core\n",
    "\n",
    "Lastly, we'll need to configure the new runtime as a [custom inference server within Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/servers/custom.html).\n",
    "Note that Seldon Core allows you to override the image used for your model at the `SeldonDeployment` level, thus **this step is not needed**.\n",
    "However, it leads to higher reusability.\n",
    "\n",
    "For our example, we will configure our new runtime under the `TFLITE_SERVER` key, by adding the following to the `predictor_servers` key in the `seldon-config` configmap:\n",
    "\n",
    "```json\n",
    "{\n",
    "     \"TFLITE_SERVER\": {\n",
    "         \"rest\": {\n",
    "             \"defaultImageVersion\": \"0.1.0-arm\",\n",
    "             \"image\": \"adriangonz/tfliteserver\"\n",
    "         }\n",
    "     }\n",
    "}\n",
    "```\n",
    "\n",
    "Note that this can be added as part of the values of the Helm chart when installing Seldon Core.\n",
    "If we assume that the chart has been installed under the `seldon-core` name in the `seldon-system` namespace, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"seldon-core\" has been upgraded. Happy Helming!\n",
      "NAME: seldon-core\n",
      "LAST DEPLOYED: Mon Oct 12 11:34:14 2020\n",
      "NAMESPACE: seldon-system\n",
      "STATUS: deployed\n",
      "REVISION: 4\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade seldon-core seldonio/seldon-core-operator \\\n",
    "    -n seldon-system \\\n",
    "    --reuse-values \\\n",
    "    --set 'predictor_servers.TFLITE_SERVER.rest.defaultImageVersion=0.1.0-arm' \\\n",
    "    --set 'predictor_servers.TFLITE_SERVER.rest.image=adriangonz/tfliteserver'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialiser Image\n",
    "\n",
    "The model initialiser is part of the Seldon Core architecture.\n",
    "It's responsible for downloading any model artifacts at init time, which will then be exposed to the actual model deployment.\n",
    "This functionality is wrapped as a Docker image, which currently only supports the `x86` architecture.\n",
    "\n",
    "Therefore, to leverage it on our edge deployment, we will need to re-build it using an ARM-compatible base.\n",
    "You can find a pre-built compatible image under tag `adriangonz/storage-initializer:v0.4.0-arm`.\n",
    "The only extra steep necessary will be to configure Seldon Core to use this image instead of the default one.\n",
    "\n",
    "As before, we can do this by changing the `seldon-config` namespace.\n",
    "We can do this by modifying the values of the Seldon Core Helm chart.\n",
    "If we assume that the chart has been installed under the `seldon-core` name in the `seldon-system` namespace, we could run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"seldon-core\" has been upgraded. Happy Helming!\n",
      "NAME: seldon-core\n",
      "LAST DEPLOYED: Mon Oct 12 11:34:22 2020\n",
      "NAMESPACE: seldon-system\n",
      "STATUS: deployed\n",
      "REVISION: 5\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade seldon-core seldonio/seldon-core-operator \\\n",
    "    -n seldon-system \\\n",
    "    --reuse-values \\\n",
    "    --set 'storageInitializer.image=adriangonz/storage-initializer:v0.4.0-arm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Now that we've got the different components set up, it should be possible to deploy our model instantiating a `SeldonDeployment` resource.\n",
    "This resource needs to specify that our machine learning deployment will:\n",
    "\n",
    "- Leverage the `TFLITE_SERVER` inference server.\n",
    "- Load the model weights from the [AIZOOTech/FaceMaskDetection repo](https://github.com/AIZOOTech/FaceMaskDetection/raw/master/models/face_mask_detection.tflite).\n",
    "- Be deployed to the node named `raspberry`, which is an edge device with an ARM architecture.\n",
    "- Specify that the model input goes into the tensor named `data_1` and the model output comes from the tensor named `cls_branch_concat_1/concat` (these are model-specific details which we've parametrised in out TFLite runtime).\n",
    "\n",
    "This can be done easily with Seldon Core as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting charts/seldondeployment-face-mask-detector.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile charts/seldondeployment-face-mask-detector.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: face-mask-detector\n",
    "  namespace: examples\n",
    "spec:\n",
    "  predictors:\n",
    "    - annotations:\n",
    "        seldon.io/no-engine: \"true\"\n",
    "      graph:\n",
    "        name: model\n",
    "        implementation: TFLITE_SERVER\n",
    "        modelUri: 'https://github.com/AIZOOTech/FaceMaskDetection/raw/master/models/face_mask_detection.tflite'\n",
    "        parameters:\n",
    "            - name: input_tensor_name\n",
    "              value: data_1\n",
    "              type: STRING\n",
    "            - name: output_tensor_name\n",
    "              value: cls_branch_concat_1/concat\n",
    "              type: STRING\n",
    "        children: []\n",
    "      componentSpecs:\n",
    "        - spec:\n",
    "            nodeName: raspberry\n",
    "      name: default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/face-mask-detector created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ./charts/seldondeployment-face-mask-detector.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
