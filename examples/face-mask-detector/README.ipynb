{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with TFLite on Edge devices\n",
    "\n",
    "This example walks through how to deploy a [TFLite model](https://www.tensorflow.org/lite/guide) in an edge device using Seldon Core.\n",
    "In particular, we will deploy a face mask detector in a Raspberry Pi, which will run inference locally from a camera feed.\n",
    "\n",
    "The face mask detector has already been pre-trained by the team at [Aizoo Tech team](https://aizoo.com) who has kindly put it available on the [AIZOOTech/FaceMaskDetection](https://github.com/AIZOOTech/FaceMaskDetection) repository. **Massive thanks to them!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seldon Core\n",
    "\n",
    "The example assumes that Seldon Core has already been installed in the cluster.\n",
    "If it hasn't, you can follow the [setup instructions in the Seldon Core documentation](https://docs.seldon.io/projects/seldon-core/en/latest/workflow/install.html).\n",
    "\n",
    "It's also worth noting that some steps below assume that Seldon Core has been installed using Helm with name `seldon-core` in the `seldon-system` namespace.\n",
    "This is not a hard requirement though, and it should be simple to adapt those steps in case it has been installed in a different location.\n",
    "\n",
    "### KubeEdge\n",
    "\n",
    "The example assumes that KubeEdge has already been installed in the cluster, exposing the `cloudcore` component so that it's accessible from an edge device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Device\n",
    "\n",
    "The example assumes that the edge device is a Raspberry Pi with an ARMv7 architecture of 32 bits.\n",
    "In particular, the example has been tested with a Raspberry Pi 3 Model B V1.2.\n",
    "We will assume that this device has been pre-synced through KubeEdge and that it's already visible as a node in the cluster with name `raspberry`. \n",
    "\n",
    "The example also requires that the Raspberry Pi has a camera module attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLite Inference Server\n",
    "\n",
    "Out of the box, Seldon Core doesn't offer support for TFLite models (particularly under an ARM architecture).\n",
    "However, as we shall see, it's fairly simple to leverage their support for [custom inference servers](https://docs.seldon.io/projects/seldon-core/en/latest/servers/custom.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Model Runtime\n",
    "\n",
    "Firstly, we'll extend the `SeldonComponent` interface to create a new model runtime.\n",
    "The methods that we'll want to extend are:\n",
    "\n",
    "- `load()`: responsible for loading our TFLite model\n",
    "- `predict()`: responsible for running inference against our model\n",
    "\n",
    "Note that we also want to parametrise the following model-specific details:\n",
    "\n",
    "- Where are model weights loaded from (i.e. `model_uri`).\n",
    "- On which tensor should we load the input data (i.e. `input_tensor_name`).\n",
    "- From which tensor should we read the model's output (i.e. `output_tensor_name`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tfliteserver/TFLiteServer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tfliteserver/TFLiteServer.py\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from seldon_core.user_model import SeldonComponent\n",
    "from tflite_runtime import interpreter as tflite\n",
    "from typing import List, Dict, Iterable\n",
    "\n",
    "TFLITE_EXT = \"*.tflite\"\n",
    "\n",
    "\n",
    "class TFLiteServer(SeldonComponent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_uri: str,\n",
    "        input_tensor_name: str,\n",
    "        output_tensor_name: str,\n",
    "    ):\n",
    "        self._model_uri = model_uri\n",
    "        self._input_tensor_name = input_tensor_name\n",
    "        self._output_tensor_name = output_tensor_name\n",
    "\n",
    "    def load(self):\n",
    "        model_path = self._get_model_path()\n",
    "        self._interpreter = tflite.Interpreter(model_path=model_path)\n",
    "        self._interpreter.allocate_tensors()\n",
    "\n",
    "        # Obtain input tensor index\n",
    "        input_tensors = self._interpreter.get_input_details()\n",
    "        self._input_tensor_index = self._get_tensor_index(\n",
    "            input_tensors, self._input_tensor_name\n",
    "        )\n",
    "\n",
    "        # Obtain output tensor index\n",
    "        output_tensors = self._interpreter.get_output_details()\n",
    "        self._output_tensor_index = self._get_tensor_index(\n",
    "            output_tensors, self._output_tensor_name\n",
    "        )\n",
    "\n",
    "    def _get_model_path(self) -> str:\n",
    "        # Search for *.tflite files to load\n",
    "        pattern = os.path.join(self._model_uri, TFLITE_EXT)\n",
    "        model_paths = glob.glob(pattern)\n",
    "        if not model_paths:\n",
    "            raise RuntimeError(f\"No models found at {self._model_uri}\")\n",
    "\n",
    "        return model_paths[0]\n",
    "\n",
    "    def _get_tensor_index(self, tensors: List[Dict], tensor_name: str) -> int:\n",
    "        for tensor in tensors:\n",
    "            if tensor[\"name\"] == tensor_name:\n",
    "                return tensor[\"index\"]\n",
    "\n",
    "        raise RuntimeError(f\"Tensor name not found: {tensor_name}\")\n",
    "\n",
    "    def predict(self, X: np.ndarray, names: Iterable[str], meta: Dict = None):\n",
    "        # Force input to be np.float32\n",
    "        img = np.float32(X)\n",
    "\n",
    "        # NOTE: This is not thread-safe!\n",
    "        self._interpreter.set_tensor(self._input_tensor_index, img)\n",
    "        self._interpreter.invoke()\n",
    "\n",
    "        output = self._interpreter.get_tensor(self._output_tensor_index)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containerise Model Runtime\n",
    "\n",
    "The next step will be building our runtime into a Docker image.\n",
    "It's worth mentioning that the new image has to be compatible with an ARM architecture, therefore we won't be able to use the [`s2i` facilities provided by Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/python/python_wrapping_s2i.html) out of the box.\n",
    "However, we can put up a together a `Dockerfile` based on [Seldon's documentation to wrap models using Docker directly](https://docs.seldon.io/projects/seldon-core/en/latest/python/python_wrapping_docker.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tfliteserver/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tfliteserver/Dockerfile\n",
    "FROM arm32v7/python:3.7.8-slim\n",
    "\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "ENV API_TYPE=\"REST\"\n",
    "ENV SERVICE_TYPE=\"MODEL\"\n",
    "\n",
    "# Install native subdeps of Python deps.\n",
    "# - `libatlas`: necessary for `numpy`\n",
    "# - `libatomic1`: necessary for `grpcio`\n",
    "RUN apt-get update \\\n",
    "    && apt-get install -y libatlas-base-dev libatomic1\n",
    "\n",
    "RUN pip install --upgrade wheel setuptools pip\n",
    "\n",
    "COPY requirements.txt ./\n",
    "RUN pip install \\\n",
    "    -r requirements.txt \\\n",
    "    --extra-index-url=https://piwheels.org/simple\n",
    "\n",
    "COPY . .\n",
    "\n",
    "CMD seldon-core-microservice TFLiteServer $API_TYPE --service-type $SERVICE_TYPE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to build this image, we'll need a host able to build `arm` images.\n",
    "In Linux, we can achieve this using QEMU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.113MB\n",
      "Step 1/10 : FROM arm32v7/python:3.7.8-slim\n",
      " ---> 4eaf7bd6a6cb\n",
      "Step 2/10 : WORKDIR /usr/src/app\n",
      " ---> Using cache\n",
      " ---> a47231aaeda8\n",
      "Step 3/10 : ENV API_TYPE=\"REST\"\n",
      " ---> Using cache\n",
      " ---> 053e4eeb29c9\n",
      "Step 4/10 : ENV SERVICE_TYPE=\"MODEL\"\n",
      " ---> Using cache\n",
      " ---> 89c2a14b355d\n",
      "Step 5/10 : RUN apt-get update     && apt-get install -y libatlas-base-dev libatomic1\n",
      " ---> Using cache\n",
      " ---> 28529c0ee508\n",
      "Step 6/10 : RUN pip install --upgrade wheel setuptools pip\n",
      " ---> Using cache\n",
      " ---> 43983be6ea0b\n",
      "Step 7/10 : COPY requirements.txt ./\n",
      " ---> Using cache\n",
      " ---> 44bbd87e8594\n",
      "Step 8/10 : RUN pip install     -r requirements.txt     --extra-index-url=https://piwheels.org/simple\n",
      " ---> Using cache\n",
      " ---> 055557535b8e\n",
      "Step 9/10 : COPY . .\n",
      " ---> 8ff71208e314\n",
      "Step 10/10 : CMD seldon-core-microservice TFLiteServer $API_TYPE --service-type $SERVICE_TYPE\n",
      " ---> Running in 39fae836c850\n",
      "Removing intermediate container 39fae836c850\n",
      " ---> d6233970d6ba\n",
      "Successfully built d6233970d6ba\n",
      "Successfully tagged adriangonz/tfliteserver:0.1.0-arm\n",
      "The push refers to repository [docker.io/adriangonz/tfliteserver]\n",
      "\n",
      "\u001b[1B5f610190: Preparing \n",
      "\u001b[1B08a6c8b1: Preparing \n",
      "\u001b[1B65293277: Preparing \n",
      "\u001b[1Bf17926cb: Preparing \n",
      "\u001b[1Be22afbe4: Preparing \n",
      "\u001b[1B0f4fcd5e: Preparing \n",
      "\u001b[1Bedd599d4: Preparing \n",
      "\u001b[1B3354b755: Preparing \n",
      "\u001b[1Bf28abf87: Preparing \n",
      "\u001b[1B3a8fa994: Preparing \n",
      "\u001b[11Bf610190: Pushed lready exists 1MB\u001b[8A\u001b[2K\u001b[9A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K0.1.0-arm: digest: sha256:430d1cf962c327caf2aaeafad3c753ebace032009e35ee251e223ba43ad95467 size: 2631\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "    ./tfliteserver \\\n",
    "    --platform linux/arm/v7 \\\n",
    "    -t adriangonz/tfliteserver:0.1.0-arm\n",
    "!docker push adriangonz/tfliteserver:0.1.0-arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new inference runtime to Seldon Core\n",
    "\n",
    "Lastly, we'll need to configure the new runtime as a [custom inference server within Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/servers/custom.html).\n",
    "Note that Seldon Core allows you to override the image used for your model at the `SeldonDeployment` level, thus **this step is not needed**.\n",
    "However, it leads to higher reusability.\n",
    "\n",
    "For our example, we will configure our new runtime under the `TFLITE_SERVER` key, by adding the following to the `predictor_servers` key in the `seldon-config` configmap:\n",
    "\n",
    "```json\n",
    "{\n",
    "     \"TFLITE_SERVER\": {\n",
    "         \"rest\": {\n",
    "             \"defaultImageVersion\": \"0.1.0-arm\",\n",
    "             \"image\": \"adriangonz/tfliteserver\"\n",
    "         }\n",
    "     }\n",
    "}\n",
    "```\n",
    "\n",
    "Note that this can be added as part of the values of the Helm chart when installing Seldon Core.\n",
    "If we assume that the chart has been installed under the `seldon-core` name in the `seldon-system` namespace, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"seldon-core\" has been upgraded. Happy Helming!\n",
      "NAME: seldon-core\n",
      "LAST DEPLOYED: Mon Oct 12 11:34:14 2020\n",
      "NAMESPACE: seldon-system\n",
      "STATUS: deployed\n",
      "REVISION: 4\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade seldon-core seldonio/seldon-core-operator \\\n",
    "    -n seldon-system \\\n",
    "    --reuse-values \\\n",
    "    --set 'predictor_servers.TFLITE_SERVER.rest.defaultImageVersion=0.1.0-arm' \\\n",
    "    --set 'predictor_servers.TFLITE_SERVER.rest.image=adriangonz/tfliteserver'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialiser Image\n",
    "\n",
    "The model initialiser is part of the Seldon Core architecture.\n",
    "It's responsible for downloading any model artifacts at init time, which will then be exposed to the actual model deployment.\n",
    "This functionality is wrapped as a Docker image, which currently only supports the `x86` architecture.\n",
    "\n",
    "Therefore, to leverage it on our edge deployment, we will need to re-build it using an ARM-compatible base.\n",
    "You can find a pre-built compatible image under tag `adriangonz/storage-initializer:v0.4.0-arm`.\n",
    "The only extra steep necessary will be to configure Seldon Core to use this image instead of the default one.\n",
    "\n",
    "As before, we can do this by changing the `seldon-config` namespace.\n",
    "We can do this by modifying the values of the Seldon Core Helm chart.\n",
    "If we assume that the chart has been installed under the `seldon-core` name in the `seldon-system` namespace, we could run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"seldon-core\" has been upgraded. Happy Helming!\n",
      "NAME: seldon-core\n",
      "LAST DEPLOYED: Mon Oct 12 11:34:22 2020\n",
      "NAMESPACE: seldon-system\n",
      "STATUS: deployed\n",
      "REVISION: 5\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade seldon-core seldonio/seldon-core-operator \\\n",
    "    -n seldon-system \\\n",
    "    --reuse-values \\\n",
    "    --set 'storageInitializer.image=adriangonz/storage-initializer:v0.4.0-arm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Now that we've got the different components set up, it should be possible to deploy our model instantiating a `SeldonDeployment` resource.\n",
    "This resource needs to specify that our machine learning deployment will:\n",
    "\n",
    "- Leverage the `TFLITE_SERVER` inference server.\n",
    "- Load the model weights from the [AIZOOTech/FaceMaskDetection repo](https://github.com/AIZOOTech/FaceMaskDetection/raw/master/models/face_mask_detection.tflite).\n",
    "- Be deployed to the node named `raspberry`, which is an edge device with an ARM architecture.\n",
    "- Specify that the model input goes into the tensor named `data_1` and the model output comes from the tensor named `cls_branch_concat_1/concat` (these are model-specific details which we've parametrised in out TFLite runtime).\n",
    "\n",
    "This can be done easily with Seldon Core as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./charts/seldondeployment-face-mask-detector.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./charts/seldondeployment-face-mask-detector.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: face-mask-detector\n",
    "  namespace: examples\n",
    "spec:\n",
    "  predictors:\n",
    "    - annotations:\n",
    "        seldon.io/no-engine: \"true\"\n",
    "      graph:\n",
    "        name: model\n",
    "        implementation: TFLITE_SERVER\n",
    "        modelUri: 'https://github.com/AIZOOTech/FaceMaskDetection/raw/master/models/face_mask_detection.tflite'\n",
    "        parameters:\n",
    "            - name: input_tensor_name\n",
    "              value: data_1\n",
    "              type: STRING\n",
    "            - name: output_tensor_name\n",
    "              value: cls_branch_concat_1/concat\n",
    "              type: STRING\n",
    "        children: []\n",
    "      componentSpecs:\n",
    "        - spec:\n",
    "            nodeName: raspberry\n",
    "      name: default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/face-mask-detector created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ./charts/seldondeployment-face-mask-detector.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Reader\n",
    "\n",
    "The next step in our example is to fetch some images to run inference on.\n",
    "For that, we will attach a camera module to our edge device through the flex port.\n",
    "This camera will be used by a new container, which will continously capture frames and run inference on them.\n",
    "\n",
    "Based on the results of each frame, the same loop will also switch on / off a set of two red and green LEDs to indicate if our model flagged someone as not wearing a mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Camera Reader\n",
    "\n",
    "For each frame, the loop will be something like:\n",
    "\n",
    "- Capture frame from camera\n",
    "- Send to our model to perform inference\n",
    "- Process the predicted values\n",
    "\n",
    "The code for the loop can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./camera-reader/camera.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./camera-reader/camera.py\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from picamera import PiCamera\n",
    "from picamera.array import PiRGBArray\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "DEBUG = os.getenv(\"DEBUG\", \"false\").lower() == \"true\"\n",
    "MODEL_SERVER = os.getenv(\"MODEL_SERVER\", default=\"http://model:9000\")\n",
    "PIXEL_FORMAT = \"RGB\"\n",
    "CAMERA_RESOLUTION = (260, 260)\n",
    "CAMERA_WARMUP_SECONDS = 2\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "def _setup_logger():\n",
    "    log_level = logging.INFO\n",
    "    if DEBUG:\n",
    "        log_level = logging.DEBUG\n",
    "    logging.basicConfig(level=log_level)\n",
    "\n",
    "\n",
    "def _get_camera() -> PiCamera:\n",
    "    logging.info(f\"Accessing camera with {CAMERA_RESOLUTION} resolution\")\n",
    "    camera = PiCamera()\n",
    "    camera.resolution = CAMERA_RESOLUTION\n",
    "    # Start a preview and let the camera warm up for 2 seconds\n",
    "    logging.info(\"Waiting for camera to warm up...\")\n",
    "    camera.start_preview()\n",
    "    time.sleep(CAMERA_WARMUP_SECONDS)\n",
    "\n",
    "    logging.info(\"Obtained camera handle!\")\n",
    "    return camera\n",
    "\n",
    "\n",
    "def _save_frame(frame: np.ndarray):\n",
    "    pass\n",
    "\n",
    "\n",
    "def _run_inference(frame: np.ndarray) -> np.ndarray:\n",
    "    logging.debug(f\"Running inference in frame with shape {frame.shape}...\")\n",
    "    \n",
    "    # Normalise pixels to [0-1] range\n",
    "    batch = np.expand_dims(frame, axis=0) / 255.0\n",
    "    payload = {\"data\": {\"ndarray\": batch.tolist()}}\n",
    "    endpoint = f\"{MODEL_SERVER}/api/v1.0/predictions\"\n",
    "\n",
    "    logging.debug(f\"Sending request to inference endpoint {endpoint}...\")\n",
    "    response = requests.post(endpoint, json=payload)\n",
    "    if not response.ok:\n",
    "        raise RuntimeError(\"Invalid frame\")\n",
    "\n",
    "    confidences = response.json()\n",
    "    logging.debug(f\"Obtained prediction with shape {confidences.shape}\")\n",
    "\n",
    "    # Filter out low-confidence predictions\n",
    "    max_confidences = np.max(confidences, axis=2)\n",
    "    classes = np.argmax(confidences, axis=2)\n",
    "    high_confidence = np.where(max_confidences > CONFIDENCE_THRESHOLD)\n",
    "\n",
    "    return high_confidence\n",
    "\n",
    "\n",
    "def _update_leds(y_pred : np.ndarray):\n",
    "    logging.debug(\"Updating LEDs...\")\n",
    "    \n",
    "    without_mask = np.count_nonzero(y_pred)\n",
    "    with_mask = len(y_pred) - without_mask\n",
    "    logging.debug(\"Detected {without_mask} persons without mask\")\n",
    "    logging.debug(\"Detected {with_mask} persons with mask\")\n",
    "    \n",
    "    # TODO: Update LEDs\n",
    "\n",
    "\n",
    "def main():\n",
    "    _setup_logger()\n",
    "    camera = _get_camera()\n",
    "    frame = PiRGBArray(camera)\n",
    "\n",
    "    logging.info(\"Starting capture loop... Smile!\")\n",
    "    for _ in camera.capture_continuous(frame, PIXEL_FORMAT.lower()):\n",
    "        if DEBUG:\n",
    "            _save_frame(frame.array)\n",
    "\n",
    "        y_pred = _run_inference(frame.array)\n",
    "        _update_leds(y_pred)\n",
    "        # Truncate to re-use\n",
    "        # https://picamera.readthedocs.io/en/release-1.13/api_array.html#pirgbarray\n",
    "        frame.truncate(0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containerise Camera Reader\n",
    "\n",
    "As before, we will containerise our code using an ARM-compatible Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./camera-reader/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./camera-reader/Dockerfile\n",
    "FROM arm32v7/python:3.7.8-slim\n",
    "\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "# Install native subdeps of Python deps.\n",
    "# - `libatlas`: necessary for `numpy`\n",
    "RUN apt-get update \\\n",
    "    && apt-get install -y libatlas-base-dev\n",
    "\n",
    "RUN pip install --upgrade wheel setuptools pip\n",
    "\n",
    "COPY requirements.txt ./\n",
    "RUN pip install \\\n",
    "    -r requirements.txt \\\n",
    "    --extra-index-url=https://piwheels.org/simple\n",
    "\n",
    "COPY . .\n",
    "\n",
    "CMD [ \"python\", \"./camera.py\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   7.68kB\n",
      "Step 1/8 : FROM arm32v7/python:3.7.8-slim\n",
      " ---> 4eaf7bd6a6cb\n",
      "Step 2/8 : WORKDIR /usr/src/app\n",
      " ---> Using cache\n",
      " ---> a47231aaeda8\n",
      "Step 3/8 : RUN apt-get update     && apt-get install -y libatlas-base-dev\n",
      " ---> Using cache\n",
      " ---> 61ef2d7db64f\n",
      "Step 4/8 : RUN pip install --upgrade wheel setuptools pip\n",
      " ---> Using cache\n",
      " ---> ecb7b782a6b3\n",
      "Step 5/8 : COPY requirements.txt ./\n",
      " ---> Using cache\n",
      " ---> bf5eabfca5b3\n",
      "Step 6/8 : RUN pip install     -r requirements.txt     --extra-index-url=https://piwheels.org/simple\n",
      " ---> Using cache\n",
      " ---> 82f17d13b920\n",
      "Step 7/8 : COPY . .\n",
      " ---> e1c5ae308faf\n",
      "Step 8/8 : CMD [ \"python\", \"./camera.py\" ]\n",
      " ---> Running in 07687afd49cb\n",
      "Removing intermediate container 07687afd49cb\n",
      " ---> 2ddc920130cc\n",
      "Successfully built 2ddc920130cc\n",
      "Successfully tagged adriangonz/camera-reader:0.1.0-arm\n",
      "The push refers to repository [docker.io/adriangonz/camera-reader]\n",
      "\n",
      "\u001b[1Bcba1bb66: Preparing \n",
      "\u001b[1Bd34deaa1: Preparing \n",
      "\u001b[1Bcf87bd0a: Preparing \n",
      "\u001b[1B40ae4cd3: Preparing \n",
      "\u001b[1Bede51fda: Preparing \n",
      "\u001b[1B0f4fcd5e: Preparing \n",
      "\u001b[1Bedd599d4: Preparing \n",
      "\u001b[1B3354b755: Preparing \n",
      "\u001b[1Bf28abf87: Preparing \n",
      "\u001b[1B3a8fa994: Preparing \n",
      "\u001b[11Bba1bb66: Pushed lready exists 9kB\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2K0.1.0-arm: digest: sha256:1742a376b86d2a186010d12421badb2d45f5be80c4fba9b7553b19f8dbdc7b38 size: 2626\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "    ./camera-reader \\\n",
    "    --platform linux/arm/v7 \\\n",
    "    -t adriangonz/camera-reader:0.1.0-arm\n",
    "!docker push adriangonz/camera-reader:0.1.0-arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy alongside our model\n",
    "\n",
    "To keep our code self-contained, we will deploy the camera reader as a sidecar container alongside our model.\n",
    "This means that the **data won't need to exit the Kubernetes pod within our edge device**.\n",
    "However, note that setup is **optional** and could be architected in a different way (e.g. if the camera was on a completely different device).\n",
    "\n",
    "To do that, we can leverage the same `SeldonDeployment` as before, including the camera reader as another container in the `componentSpecs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./charts/seldondeployment-face-mask-detector-with-camera-reader.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./charts/seldondeployment-face-mask-detector-with-camera-reader.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: face-mask-detector\n",
    "  namespace: examples\n",
    "spec:\n",
    "  predictors:\n",
    "    - annotations:\n",
    "        seldon.io/no-engine: \"true\"\n",
    "      graph:\n",
    "        name: model\n",
    "        implementation: TFLITE_SERVER\n",
    "        modelUri: \"https://github.com/AIZOOTech/FaceMaskDetection/raw/master/models/face_mask_detection.tflite\"\n",
    "        parameters:\n",
    "          - name: input_tensor_name\n",
    "            value: data_1\n",
    "            type: STRING\n",
    "          - name: output_tensor_name\n",
    "            value: cls_branch_concat_1/concat\n",
    "            type: STRING\n",
    "        children: []\n",
    "      componentSpecs:\n",
    "        - spec:\n",
    "            containers:\n",
    "              - name: camera-reader\n",
    "                image: adriangonz/camera-reader:0.1.0-arm\n",
    "                volumeMounts:\n",
    "                  - mountPath: /opt/vc\n",
    "                    name: vc-libs\n",
    "                  - mountPath: /dev/vchiq\n",
    "                    name: dev-vchiq\n",
    "                securityContext:\n",
    "                  privileged: true\n",
    "                  runAsUser: 0\n",
    "                env:\n",
    "                  - name: MODEL_SERVER\n",
    "                    value: http://172.17.0.2:9001\n",
    "                  - name: DEBUG\n",
    "                    value: 'true'\n",
    "                  - name: LD_LIBRARY_PATH\n",
    "                    value: /opt/vc/lib\n",
    "            volumes:\n",
    "              - hostPath:\n",
    "                  path: /opt/vc\n",
    "                  type: \"\"\n",
    "                name: vc-libs\n",
    "              - hostPath:\n",
    "                  path: /dev/vchiq\n",
    "                  type: \"\"\n",
    "                name: dev-vchiq\n",
    "            nodeName: raspberry\n",
    "      name: default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/face-mask-detector created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ./charts/seldondeployment-face-mask-detector-with-camera-reader.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
